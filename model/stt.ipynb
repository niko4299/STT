{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPQdj_CXpVjH"
      },
      "source": [
        "!pip install torch>=1.2.0\n",
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfWGiSerlD_t"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OU39qpTlGvz"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "      char_map_str = \"\"\"\n",
        "      ' 0\n",
        "      <SPACE> 1\n",
        "\t\t  a 2\n",
        "\t\t  b 3\n",
        "\t\t  c 4\n",
        "\t\t  d 5\n",
        "\t\t  e 6\n",
        "\t\t  f 7\n",
        "\t\t  g 8\n",
        "\t\t  h 9\n",
        "\t\t  i 10\n",
        "\t\t  j 11\n",
        "\t\t  k 12\n",
        "\t\t  l 13\n",
        "\t\t  m 14\n",
        "\t\t  n 15\n",
        "\t\t  o 16\n",
        "\t\t  p 17\n",
        "\t\t  q 18\n",
        "\t\t  r 19\n",
        "\t\t  s 20\n",
        "\t\t  t 21\n",
        "\t\t  u 22\n",
        "\t\t  v 23\n",
        "\t\t  w 24\n",
        "\t\t  x 25\n",
        "\t\t  y 26\n",
        "\t\t  z 27\n",
        "\t\t  \"\"\"\n",
        "\n",
        "      self.char_map = {}\n",
        "      self.index_map = {}\n",
        "      for line in char_map_str.strip().split('\\n'):\n",
        "          ch, index = line.split()\n",
        "          self.char_map[ch] = int(index)\n",
        "          self.index_map[int(index)] = ch\n",
        "      self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=81),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "  spectrograms = []\n",
        "  labels = []\n",
        "  input_lengths = []\n",
        "  label_lengths = []\n",
        "  for (waveform, _, utterance, _, _, _) in data:\n",
        "    if data_type == 'train':\n",
        "      spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "    elif data_type == 'valid':\n",
        "      spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "    else:\n",
        "      raise Exception('data_type should be train or valid')\n",
        "    spectrograms.append(spec)\n",
        "    label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "    labels.append(label)\n",
        "    input_lengths.append(spec.shape[0]//2)\n",
        "    label_lengths.append(len(label))\n",
        "\n",
        "  spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "  labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "  return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swgBshBslNnU"
      },
      "source": [
        "class ActDropNormCNN1D(nn.Module):\n",
        "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
        "        super(ActDropNormCNN1D, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(n_feats)\n",
        "        self.keep_shape = keep_shape\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.norm(self.dropout(F.gelu(x)))\n",
        "        x = self.dropout(F.gelu(self.norm(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognition(nn.Module):\n",
        "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
        "        super(SpeechRecognition, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
        "            ActDropNormCNN1D(n_feats, dropout),\n",
        "        )\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(n_feats, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, dropout=0.0,\n",
        "                            bidirectional=False)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.final_fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        n, hs = self.num_layers, self.hidden_size\n",
        "        return (torch.zeros(n*1, batch_size, hs),\n",
        "                torch.zeros(n*1, batch_size, hs))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.squeeze(1)\n",
        "        x = self.cnn(x) \n",
        "        x = self.dense(x) \n",
        "        x = x.transpose(0, 1) \n",
        "        out, (hn, cn) = self.lstm(x, hidden)\n",
        "        x = self.dropout2(F.gelu(self.layer_norm2(out)))\n",
        "        return self.final_fc(x), (hn, cn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVwEDVFRo47i"
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0IPUDRfmd9f"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "      spectrograms, labels, input_lengths, label_lengths = _data \n",
        "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "      bs = spectrograms.shape[0]\n",
        "      hidden = model._init_hidden(bs)\n",
        "      hn, c0 = hidden[0].to(device), hidden[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output,_ = model(spectrograms, (hn, c0))\n",
        "      output = F.log_softmax(output, dim=2) \n",
        "\n",
        "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      iter_meter.step()\n",
        "      if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(spectrograms), data_len,100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5Ow0E4mmBV"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "if not os.path.isdir(\"./data\"):\n",
        "  os.makedirs(\"./data\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=20,\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMjzAKaTm7RJ"
      },
      "source": [
        "hyper_parameters = {\n",
        "        \"num_classes\": 29,\n",
        "        \"n_feats\": 81,\n",
        "        \"dropout\": 0.1,\n",
        "        \"hidden_size\": 1024,\n",
        "        \"num_layers\": 1\n",
        "    }\n",
        "\n",
        "model = SpeechRecognition(**hyper_parameters).to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), 0.001)\n",
        "criterion = nn.CTCLoss(blank=28).to(device)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=10,\n",
        "                                            anneal_strategy='linear')\n",
        "\n",
        "iter_meter = IterMeter()\n",
        "for epoch in range(1, 10 + 1):\n",
        "  train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
        "\n",
        "torch.save(model.state_dict(), \"./drive/MyDrive/model_one.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}